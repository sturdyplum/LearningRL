Date:
  1/29/2019

Summary:
  This is my first week in which I start to make a serious attempt at learning reinforcment learning. While in the past I have studied it a decent amount, I have never studied it in a formal way and never really made an effort to reinforce everything I am learning. The reason I start now is because of my senior design project as well as having a large amount of free time on my hands. This week I began working through a course hosted on github [https://github.com/dennybritz/reinforcement-learning], this goes through an online course as well as a textbook, providing help and extersizes through the way.

Progress:
  I have worked through the "Introduction to RL problems & OpenAI Gym" and "MDPs and Bellman Equations" sections of the course. SO far the only assignment has been to go through the openAi gym tutorial. This was pretty fast and not really worth posting my code or anything. While I have seen a large amount of these topics before, I have never seen them so formaly defined and it has made my overall understanding better.
  
Things Learned:
  The beggining of this course seems highly centered around MDPs as well as the Bellman Equations and breaking down RL into formals maths. Specifically I was introduced into the notion of RL being made up of functions, primarily the value function and the policy funciton. The differences in models that use one or the other or both are explored a bit but nothing to deep. Overall the whole thing seems to be wrapped in a MDP, a mathematical concept which reminds me heavily, and to no surprise is also deeply related to, dynamic programming.
